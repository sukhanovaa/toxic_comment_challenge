# toxic comments challenge 
https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge

## Что было сделано
### Сбор/аугментация данных
1. src/add_from_wiki.py - С помощью [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/2) были получены эмбеддинги данных с [Wikipedia Talk Corpus](https://figshare.com/articles/Wikipedia_Talk_Corpus/4264973) за 2010-2015 гг. Эмбеддинги обучающих данных были использованы в качестве базы для kNN, новые данные были размечены с помощью label propagation (мода NN = 5; для меньшего NN метки всё меньше походили на правду, для большего практически все данные оказывались нейтральными)
2. src/augmenting.py - В качестве аугментации случайное количество слов в каждом комментарии было с помощью синсетов из WordNet заменено на соответствующие синонимы (см. н. https://www.aclweb.org/anthology/N18-2072, тж. https://nesa.zju.edu.cn/download/TEXTBUGGER%20Generating%20Adversarial%20Text%20Against%20Real-world%20Applications.pdf, где синонимичные замены рассматриваются как атаки/adversarial attacks), что увеличило объём данных в 2 раза (можно было и больше) и предположительно повысило устойчивость/способность к генерализации
Всего около 400,000 комментариев.
### Модель (src/modeling.py, src/training.py)
1. На данных был дообучен слой над основной моделью BERT. Поскольку у меня нет TPU, а Colaboratory неустойчиво работает, я дотянула модель только до ROC-AUC 0.85, но вообще это вопрос времени и ресурсов (см. https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d, где был скор выше 0.95)
2. Чтобы получить намного меньшую по объёму модель, было применено knowledge distillation. Дообученный BERT выступил как модель-учитель, [классификатор на основе CNN - как модель-студент](https://openreview.net/pdf?id=HJxM3hftiX), часть Wikipedia Talk Corpus за 2010 г. - в качестве дополнительных неразмеченных данных. При a=0.7, T=1 через пару эпох ROC-AUC на тесте был 0.84. Дальше обучать особо нет смысла, т.к. из-за того, что в незамеченных данных полностью нейтральных комментариев во много раз больше (это было проверено ещё в ходе label propagation), случается сильное переобучение
### Фронтенд (toxic-onnx)
Экспорт модели в ONNX прошёл успешно, но дальше ни на [onnx-js](https://github.com/microsoft/onnxjs/), ни на [onnx-tensorflow](https://github.com/onnx/onnx-tensorflow) модель не заработала. Фронтенд я не доделала.  

## Что можно было сделать и улучшить
### Данные
Можно и целесообразно было бы выйти за пределы домена комментариев к статьям Википедии и попробовать доразметить также комментарии откуда угодно - с реддита, твиттера и т.д. Однако для того, чтобы гарантированно найти близкие и явно токсичные комментарии, чтобы досемплить именно их, а не полностью нейтральный класс, мне потребовалось бы явно больше ресурсов, чтобы работать с такими большими данными.
Другой идеей было попробовать построить CNN на символах, чтобы бороться с опечатками (опять же, это могло бы дать устойчивость модели к атакам), но даже с большим количеством параметров такая сеть быстро переобучалась.
### Модель
На самом деле, первоначальной моей идеей было создание собственного трансформера и если не прунинг/сжатие, то использование его в качестве модели-учителя. Но поскольку обучающих данных для трансформера крайне мало, здесь нужны были бы хорошие предобученные эмбеддинги, какие упоминаются у победителя челленджа. Я построила glove на 100,000 BPE-токенов из Wikipedia Talk Corpus, но это дало, при всех гиперпараметрах (+ синонимичные замены), ROC-AUC около 0.75-0.8, а с большими данными типа Википедии, CommonCrawl и т.д. у меня не было ресурсов для их обработки.
### Фронтенд
Идейно правильно с самого начала было писать на Tensorflow. 
Погрешив на то, что JS не понимает int64 (это тип эмбеддингов в PyTorch, откуда конвертировалась модель), я даже конвертировала модель без них, оставив эмбеддинги внешним словарём, но ничего не сработало. Конвертация в tfjs оказалась очень болезненным предприятием, из которого тоже ничего не вышло, спасибо обновлённому API Tensorflow, не поддерживающему конвертацию в tfjs из frozen graph (которым технически оказывается модель после PyTorch → ONNX и ONNX → Tensorflow).

##Остальное
src/preprocessing.py - предобработка текста для обучения glove
src/utils.py - для работы с данными. Я не использовала ни pd.Dataframe, ни даже csv.DictReader, только простой текст.